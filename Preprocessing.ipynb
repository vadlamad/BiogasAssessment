{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8e668-802f-40ff-8f90-f752b9c56be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module: biogas_pipeline\n",
    "Description: Processes sensor data from biogas installations by performing data quality checks,\n",
    "preprocessing sensor readings (flow rate and methane concentration), and merging additional metadata.\n",
    "Handles S3 data reads and writes with retry logic.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import awswrangler as wr\n",
    "from botocore.exceptions import ClientError, EndpointConnectionError\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BiogasPreprocessor:\n",
    "    \"\"\"\n",
    "    Class to preprocess biogas sensor data.\n",
    "\n",
    "    Attributes:\n",
    "        methane_col (str): Column name for methane concentration.\n",
    "        flow_col (str): Column name for flow rate.\n",
    "        timestamp_col (str): Column name for the timestamp.\n",
    "        max_methane (float): Maximum allowed methane percentage.\n",
    "        max_flow (float): Maximum allowed flow rate.\n",
    "        rolling_window (int): Window size for rolling median smoothing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        methane_col: str = 'methane_percent',\n",
    "        flow_col: str = 'flow_rate',\n",
    "        timestamp_col: str = 'timestamp',\n",
    "        max_methane: float = 100,\n",
    "        max_flow: float = 500,\n",
    "        rolling_window: int = 5\n",
    "    ) -> None:\n",
    "        self.methane_col = methane_col\n",
    "        self.flow_col = flow_col\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.max_methane = max_methane\n",
    "        self.max_flow = max_flow\n",
    "        self.rolling_window = rolling_window\n",
    "\n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess the input dataframe:\n",
    "          - Convert timestamp column to datetime and sort the values.\n",
    "          - Compute duration between sensor readings.\n",
    "          - Clip and fill missing values for methane and flow rate.\n",
    "          - Apply rolling median smoothing.\n",
    "          - Calculate energy output in BTU.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input sensor data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed sensor data.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "\n",
    "        # Convert and sort timestamps\n",
    "        df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col], errors='coerce', utc=True)\n",
    "        df = df.sort_values(self.timestamp_col)\n",
    "        print(\"#@\"*20 + \"Timestamp col is:\")\n",
    "        print(df[self.timestamp_col].dtypes)\n",
    "\n",
    "        # Calculate duration between readings (in minutes)\n",
    "        df['duration_min'] = df[self.timestamp_col].diff().dt.total_seconds() / 60\n",
    "        df['duration_min'] = df['duration_min'].fillna(1)\n",
    "\n",
    "        # Clip sensor readings to valid ranges\n",
    "        df['methane_percent'] = df[self.methane_col].clip(lower=0, upper=self.max_methane)\n",
    "        df['flow_rate'] = df[self.flow_col].clip(lower=0, upper=self.max_flow)\n",
    "\n",
    "        # Fill missing sensor values using forward and backward fill methods\n",
    "        df['methane_percent'] = df['methane_percent'].fillna(method='ffill').fillna(method='bfill')\n",
    "        df['flow_rate'] = df['flow_rate'].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "        # Apply rolling median smoothing for sensor readings\n",
    "        df['methane_smooth'] = (\n",
    "            df['methane_percent']\n",
    "            .rolling(window=self.rolling_window, center=True)\n",
    "            .median()\n",
    "            .fillna(method='bfill')\n",
    "            .fillna(method='ffill')\n",
    "        )\n",
    "        df['flow_smooth'] = (\n",
    "            df['flow_rate']\n",
    "            .rolling(window=self.rolling_window, center=True)\n",
    "            .median()\n",
    "            .fillna(method='bfill')\n",
    "            .fillna(method='ffill')\n",
    "        )\n",
    "\n",
    "        # Compute the energy output in BTU (using a constant conversion factor 1010)\n",
    "        df['energy_output_btu'] = (\n",
    "            df['flow_smooth'] *\n",
    "            df['duration_min'] *\n",
    "            (df['methane_smooth'] / 100) *\n",
    "            1010\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def retry_s3_operation(\n",
    "    func: Any, retries: int = 3, delay: int = 5, *args, **kwargs\n",
    ") -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Attempts an S3 operation with retry logic.\n",
    "\n",
    "    Args:\n",
    "        func (callable): The function to be retried.\n",
    "        retries (int): Number of retry attempts.\n",
    "        delay (int): Delay (in seconds) between retry attempts.\n",
    "        *args: Additional positional arguments for the function.\n",
    "        **kwargs: Additional keyword arguments for the function.\n",
    "\n",
    "    Returns:\n",
    "        The result of the function if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            if len(args) == 0:\n",
    "                return func\n",
    "            else:\n",
    "                logger.info(\"Reading a json file\")\n",
    "        except (ClientError, EndpointConnectionError) as e:\n",
    "            logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(delay)\n",
    "    logger.error(\"All retry attempts failed.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def run_data_quality_checks(\n",
    "    df: pd.DataFrame, timestamp_col: str = 'timestamp'\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run basic data quality checks on the sensor data and display the findings.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The sensor data.\n",
    "        timestamp_col (str): The name of the timestamp column.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A report dictionary containing shape, missing values, low variance columns,\n",
    "                        duplicate timestamp counts, timestamp gaps, and outlier counts.\n",
    "    \"\"\"\n",
    "    display(Markdown(\"### 🔍 Data Quality Report\"))\n",
    "    report: Dict[str, Any] = {}\n",
    "\n",
    "    # ------------------------------\n",
    "    # Exploratory Data Analysis (EDA)\n",
    "    # ------------------------------\n",
    "\n",
    "    # 1. Basic Information: shape, first rows, and data types.\n",
    "    # Report dataframe shape\n",
    "    report['shape'] = df.shape\n",
    "    display(Markdown(f\"**Data shape:** {df.shape}\"))\n",
    "\n",
    "    # report['head'] = df.head().to_markdown(index=False)\n",
    "    # display(Markdown(f\"**Data Head Rows:** {df.head()}\"))\n",
    "\n",
    "    # Provides details on column types and non-null counts\n",
    "    # report['info'] = df.info()\n",
    "    # display(Markdown(f\"**DataFrame Info:** {df.info()}\"))\n",
    "\n",
    "    # # 2. Descriptive statistics for numeric columns.\n",
    "    # report['descriptive_stats'] = df.describe()\n",
    "    # display(Markdown(f\"**Descriptive Statistics::** {df.describe()}\"))\n",
    "\n",
    "    # 3. Check for missing values in each column.\n",
    "    report['missing values by column'] = df.isnull().sum()\n",
    "    # display(Markdown(f\"**Missing Values by Column:::** {df.isnull().sum()}\"))\n",
    "\n",
    "    all_null_report = {col: df[col].isnull().all() for col in df.columns}\n",
    "    true_keys = [(k, v) for k, v in all_null_report.items() if v is True]\n",
    "    display(Markdown(f\"**Missing all rows by Column:::** {true_keys}\"))\n",
    "\n",
    "    # # 4. Histogram plots for numeric columns.\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    # for col in numeric_cols:\n",
    "    #     plt.figure()\n",
    "    #     df[col].hist()\n",
    "    #     plt.title(f'Histogram of {col}')\n",
    "    #     plt.xlabel(col)\n",
    "    #     plt.ylabel('Frequency')\n",
    "    #     plt.show()\n",
    "\n",
    "    # # 5. Value counts for categorical (object type) columns.\n",
    "    # categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    # print(\"\\nValue Counts for Categorical Columns:\")\n",
    "    # for col in categorical_cols:\n",
    "    #     print(f\"\\nValue counts for column '{col}':\")\n",
    "    #     print(df[col].value_counts())\n",
    "\n",
    "    # 6. Optional: Plot a correlation matrix if there are multiple numeric variables.\n",
    "    # if len(numeric_cols) > 1:\n",
    "    #     plt.figure()\n",
    "    #     corr_matrix = df.corr()\n",
    "    #     # Display the correlation matrix using matshow.\n",
    "    #     plt.matshow(corr_matrix, fignum=1)\n",
    "    #     plt.title('Correlation Matrix', pad=20)\n",
    "    #     plt.colorbar()\n",
    "    #     plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
    "    #     plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "    #     plt.show()\n",
    "\n",
    "    #  7. Calculate missing value percentage for each column\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_report = missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
    "    display(Markdown(\"**Missing Values (%):**\"))\n",
    "    display(missing_report)\n",
    "    report['missing_values'] = missing_report.to_dict()\n",
    "\n",
    "    # 8. Identify columns with low variance (0 or 1 unique value) - Flat sensors are uninformative and can be dropped.\n",
    "    low_var_cols = df.loc[:, df.nunique() <= 1].columns.tolist()\n",
    "    display(Markdown(\"**Low Variance Columns:**\"))\n",
    "    display(low_var_cols)\n",
    "    report['low_variance_columns'] = low_var_cols\n",
    "\n",
    "    # 9. Timestamp related checks\n",
    "    if timestamp_col in df.columns:\n",
    "        duplicates = df.duplicated(subset=timestamp_col).sum()\n",
    "        display(Markdown(f\"**Duplicate Timestamps:** {duplicates}\"))\n",
    "        report['duplicate_timestamps'] = int(duplicates)\n",
    "\n",
    "        df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "        time_deltas = df[timestamp_col].diff().dt.total_seconds()\n",
    "        display(Markdown(\"**Timestamp Gaps (in seconds):**\"))\n",
    "        display(time_deltas.describe())\n",
    "        report['timestamp_gaps'] = time_deltas.describe().to_dict()\n",
    "\n",
    "    # 10. Out-of-range sensor checks (e.g., negative values)\n",
    "    outliers: Dict[str, int] = {}\n",
    "    if 'methane_percent' in df.columns:\n",
    "        neg_ch4 = (df['methane_percent'] < 0).sum()\n",
    "        outliers['negative_ch4'] = int(neg_ch4)\n",
    "    if 'flow_rate' in df.columns:\n",
    "        neg_flow = (df['flow_rate'] < 0).sum()\n",
    "        outliers['negative_flow'] = int(neg_flow)\n",
    "    display(Markdown(\"**Out-of-Range Sensor Checks:**\"))\n",
    "    display(outliers)\n",
    "    report['out_of_range'] = outliers\n",
    "\n",
    "    # 11. Visualize correlation between numeric sensor variables using a heatmap. Helps detect redundant \n",
    "    # or correlated features\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    if not numeric_cols.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(numeric_cols.corr(), annot=False, cmap='coolwarm')\n",
    "        plt.title(\"Sensor Correlation Heatmap\")\n",
    "        plt.show()\n",
    "\n",
    "    return report\n",
    "\n",
    "def process_bool_columns(df: pd.DataFrame):\n",
    "    # Step 1: Find actual boolean columns\n",
    "    #bool_cols = df.select_dtypes(include='bool').columns.tolist()\n",
    "    result = {}\n",
    "    for col in df.columns:\n",
    "        unique_vals = set(df[col].dropna().unique())\n",
    "        if unique_vals.issubset({True, False, 0.0, 1.0, 0, 1}):\n",
    "            result[col] = unique_vals\n",
    "    bool_cols = list(result.keys())\n",
    "\n",
    "    # Step 2: Find object columns that contain true, false, 0, 1, 0.0, or 1.0 (but don't drop rows!). \n",
    "    # And also convert to datatype boolean\n",
    "    true_set = {\"true\", \"yes\", \"1\", \"1.0\"}\n",
    "    false_set = {\"false\", \"no\", \"0\", \"0.0\"}\n",
    "\n",
    "    for col in bool_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype(str).str.strip().str.lower().map(\n",
    "                lambda x: True if x in true_set\n",
    "                else False if x in false_set else pd.NA\n",
    "            ).astype('boolean')\n",
    "\n",
    "\n",
    "    # Step 3: Print value counts for existing boolean columns\n",
    "    # print(\"🔍 Value counts for boolean columns:\")\n",
    "    # for col in bool_cols:\n",
    "    #     print(f\"\\n{col}:\")\n",
    "    #     print(df[col].dtype)\n",
    "    #     print(df[col].value_counts())\n",
    "\n",
    "    all_bool_cols = list(set(bool_cols))\n",
    "\n",
    "    return df, all_bool_cols\n",
    "\n",
    "def standardize_column_types(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes mixed-type object columns in the DataFrame:\n",
    "    - Converts to numeric if >50% of values can be converted\n",
    "    - Otherwise, trims strings and keeps them as type `str`\n",
    "    - Boolean columns are left untouched\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        verbose (bool): If True, prints conversion steps\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with standardized column types\n",
    "    \"\"\"\n",
    "    df_standardized = df.copy()\n",
    "\n",
    "    for col in df_standardized.columns:\n",
    "        col_dtype = df_standardized[col].dtype\n",
    "\n",
    "        # Skip boolean columns\n",
    "        if col_dtype == 'bool':\n",
    "            if verbose:\n",
    "                print(f\"🛑 Column '{col}' is boolean. Skipping standardization.\")\n",
    "            continue\n",
    "\n",
    "        # Handle object (likely mixed) columns\n",
    "        if col_dtype == 'object':\n",
    "            num_converted = pd.to_numeric(df_standardized[col], errors='coerce')\n",
    "            if num_converted.notnull().sum() > len(df_standardized[col]) / 2:\n",
    "                df_standardized[col] = num_converted\n",
    "                if verbose:\n",
    "                    print(f\"📊 Column '{col}' standardized to numeric.\")\n",
    "            else:\n",
    "                df_standardized[col] = df_standardized[col].astype(str).str.strip()\n",
    "                if verbose:\n",
    "                    print(f\"🔤 Column '{col}' retained as string (stripped).\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"✅ Column '{col}' already {col_dtype}. No change.\")\n",
    "\n",
    "    return df_standardized\n",
    "\n",
    "def merge_facility_dfs(\n",
    "    df_1: pd.DataFrame,\n",
    "    df_2: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges two facilities data.\n",
    "\n",
    "    Args:\n",
    "         df_1[pd.DataFrame]: Facility_2 dataframe.\n",
    "         df_2[pd.DataFrame]: Facility_2 dataframe.\n",
    "\n",
    "    Returns:\n",
    "         pd.DataFrame: Cleaned data types in facility 2 dataframe\n",
    "    \"\"\"\n",
    "    # # Columns (1,10,11,17,18,19,20,23,24,25,29,30,31,36,37,38,40,41,42,45,46,47,48,\n",
    "    # #          54,55,56,58,59,60,61,62,63,67,68,69,72,82,84,85) have mixed types. \n",
    "\n",
    "    # ------------------------------\n",
    "    # Union of Columns\n",
    "    # ------------------------------\n",
    "    union_columns = list(set(df_1.columns.tolist()) | set(df_2.columns.tolist()))\n",
    "    df1_union = df_1.reindex(columns=union_columns)\n",
    "    df2_union = df_2.reindex(columns=union_columns)\n",
    "    combined_union = pd.concat([df1_union, df2_union], ignore_index=True)\n",
    "\n",
    "    return combined_union\n",
    "\n",
    "\n",
    "def load_merge_csv_json(\n",
    "    csv_s3_path: str,\n",
    "    json_s3_path: str,\n",
    "    facility_value: str\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads sensor CSV data and merges it with facility JSON metadata from S3.\n",
    "\n",
    "    Args:\n",
    "        csv_s3_path (str): S3 path to the CSV file containing sensor data.\n",
    "        json_s3_path (str): S3 path to the JSON file containing facility metadata.\n",
    "        facility_value (str): Facility identifier to be added to the dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with sensor data and metadata, or None if any error occurs.\n",
    "    \"\"\"\n",
    "    df_1 = pd.DataFrame()\n",
    "    df_2 = pd.DataFrame()\n",
    "    # Read CSV data from S3 with retry logic\n",
    "    df = retry_s3_operation(wr.s3.read_csv(csv_s3_path, low_memory=False))\n",
    "    if df is None:\n",
    "        logger.error(f\"Failed to load CSV from {csv_s3_path} after retries.\")\n",
    "        return None\n",
    "\n",
    "    # Load metadata JSON from S3 and extract coordinates\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        bucket, key = json_s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "        response = retry_s3_operation(s3_client.get_object(Bucket=bucket, Key=key))\n",
    "        if response is None:\n",
    "            logger.error(f\"Failed to load JSON from {json_s3_path} after retries.\")\n",
    "            return None\n",
    "        coords = json.loads(response['Body'].read())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to parse JSON from {json_s3_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Merge facility metadata into the dataframe\n",
    "    df['longitude'] = coords.get('longitude')\n",
    "    df['latitude'] = coords.get('latitude')\n",
    "    df['site_comm_date'] = coords.get('site_comm_date')\n",
    "    df['facility'] = facility_value\n",
    "\n",
    "    if (facility_value == \"facility_1\"):\n",
    "        df_1 = df.copy()\n",
    "    elif (facility_value == \"facility_2\"):\n",
    "        df_2 = df.copy()\n",
    "\n",
    "    combined_df = merge_facility_dfs(df_1, df_2)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to process sensor data from multiple facilities.\n",
    "      - Loads data from S3.\n",
    "      - Runs data quality checks.\n",
    "      - Preprocesses the sensor data.\n",
    "      - Saves the processed data back to S3.\n",
    "    \"\"\"\n",
    "    facilities = ['facility_1', 'facility_2']\n",
    "    preprocessor = BiogasPreprocessor()\n",
    "    base_s3_path = 's3://sagemaker-us-east-2-426179662034'\n",
    "    dataframes = []\n",
    "    output_path = f'{base_s3_path}/canvas/processed/facility_merge_processed.csv'\n",
    "\n",
    "    for facility in tqdm(facilities, desc=\"Processing Facilities\"):\n",
    "        csv_path = f'{base_s3_path}/canvas/{facility}/{facility}_data.csv'\n",
    "        json_path = f'{base_s3_path}/canvas/{facility}/{facility}_coordinates.json'\n",
    " \n",
    "        logger.info(f\"\\nProcessing {facility.upper()}\\n{'=' * 50}\")\n",
    "\n",
    "        df = load_merge_csv_json(csv_path, json_path, facility_value=facility)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    df_merged = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Little cleanup to make the headernames readable\n",
    "    # Remove 'bop_plc_' suffix from column names\n",
    "    df_merged.columns = df_merged.columns.str.replace('bop_plc_', '')\n",
    "\n",
    "    # Rename selected columns\n",
    "    df_merged_renamed = df_merged.rename(columns={\n",
    "    'abb_gc_outletstream_ch4': 'methane_percent',\n",
    "    'abb_gc_outletstream_flow': 'flow_rate',\n",
    "    'bge_skid_running': 'operational'\n",
    "    })\n",
    "\n",
    "    if df_merged_renamed is None:\n",
    "        logger.warning(f\"Skipping {facility} due to previous errors.\")\n",
    "\n",
    "    # # Run and display data quality checks . Just commented out not to run out of memory while testing.\n",
    "    run_data_quality_checks(df_merged_renamed)\n",
    "\n",
    "    df_processed_cleaned_bool, all_bool_cols = process_bool_columns(df_merged_renamed)\n",
    "    bool_value_counts = {col: df_processed_cleaned_bool[col].value_counts(dropna=False) for col in all_bool_cols}\n",
    "\n",
    "    # print all of them\n",
    "    print(bool_value_counts)\n",
    "\n",
    "    df_processed_standardized = standardize_column_types(df_processed_cleaned_bool, True)\n",
    "\n",
    "    # Preprocess sensor data\n",
    "    df_preprocess_completed = preprocessor.preprocess(df_processed_standardized)\n",
    "    low_var_cols = df_preprocess_completed.loc[:, df_preprocess_completed.nunique() <= 1].columns.tolist()\n",
    "    df_preprocess_completed = df_preprocess_completed.drop(low_var_cols, axis=1)\n",
    "\n",
    "    numeric_cols = df_preprocess_completed.select_dtypes(include=[np.number])\n",
    "    if not numeric_cols.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(numeric_cols.corr(), annot=False, cmap='coolwarm')\n",
    "        plt.title(\"Sensor Correlation Heatmap\")\n",
    "        plt.show()\n",
    "\n",
    "    df_preprocess_completed_sorted = df_preprocess_completed.sort_values(by='timestamp')\n",
    "\n",
    "    # # Get first and last n rows for each facility\n",
    "    # first = df_preprocess_completed_sorted.groupby('facility').head(12500)\n",
    "    # last = df_preprocess_completed_sorted.groupby('facility').tail(12500)\n",
    "\n",
    "    # # Combine and drop duplicates (in case there's overlap)\n",
    "    # result = pd.concat([first, last]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # print(result.shape)\n",
    "    # row_counts = result.groupby('facility').size()\n",
    "    # print(row_counts)\n",
    "\n",
    "    # Save the processed data back to S3\n",
    "    try:\n",
    "        retry_s3_operation(wr.s3.to_csv(df_preprocess_completed_sorted, path=output_path, index=False))\n",
    "        logger.info(f\"Processed data saved to: {output_path}\\n\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to write processed data to {output_path}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
